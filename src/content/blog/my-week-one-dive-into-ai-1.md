---
title: "My Week 1 Dive into Bayesâ€™ Theorem and Vectorization"
description: "Exploring how probability, classification, and text vectorization build the foundation for AI understanding."
pubDate: 2025-05-07
author:  "Jim Shelly"
tags: ["AI", "Machine Learning", "Naive Bayes", "Vectorization", "NLP"]
image: "/images/blog/week1_1.png"
---

## ğŸš€ Launching My AI Learning Journey

In my first week of studying for the CAIEâ„¢ certification, I dove headfirst into two foundational AI conceptsâ€”**Bayesâ€™ Theorem** and **Vectorization (Bag-of-Words)**. Both are essential for understanding how early AI systems classify text and make probabilistic decisions.

---

## ğŸ” Bayesâ€™ Theorem in Action

Bayesâ€™ Theorem allows us to update our beliefs based on new information. Itâ€™s especially useful in text classification tasks like spam filtering.

**Formula Recap**:
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]

### ğŸ“§ Example:
If the word **â€œfreeâ€** appears in 30% of spam emails and 5% of non-spam, and 40% of all emails are spam, whatâ€™s the probability an email is spam if it contains **â€œfreeâ€**?

**Answer**: 80%â€”meaning the word strongly signals spam!

I solved several of these by hand and truly saw how probabilities shift with evidence.

---

## ğŸ§  Naive Bayes Classifier

Naive Bayes builds on this concept to classify documents:

- **Naive Assumption**: Features (words) are conditionally independent.
- **Use Case**: Spam detection, sentiment analysis, topic categorization.
- **Tool**: I trained a `MultinomialNB` model in Python using `scikit-learn`.

```python
from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()
model.fit(X, labels)
